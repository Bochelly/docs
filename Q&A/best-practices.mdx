---
title: 'Best Practices'
icon: 'circle-check'
iconType: 'solid'
---

A/B testing wording can dramatically enhance user engagement and conversion rates. Leading tech-savvy companies have launched their own A/B testing tools over recent years, conducting thousands of experiments annually.

<Tip>
    Did you know that Netflix continuously runs on average 20 A/B tests for every user at any given time?<br/>
    <i>Learn more at the [Netflix Technology Blog](https://netflixtechblog.medium.com)</i>
</Tip>

To ensure your experiments are both effective and insightful, adhere to these best practices.

---
## Defining goals

### What is the objective of your A/B test?
Clearly define the objective of your experiment. Common goals include increasing click-through rates, improving conversion rates, and reducing bounce rates. A well-defined goal helps design relevant variations and measure success accurately.

While A/B testing is commonly used to increase revenue, it can also be employed for various other objectives:
- Increasing upsells
- Reducing opt-out rates
- Reducing after-sales contact
- Improving community engagement
- Reducing time to purchase

<Tip>
    Don't just focus on increasing revenue. A/B testing can also significantly help in reducing your costs. <br/>
    Discover more through our [user stories.](/Documentation/Examples)
</Tip>

### How will you measure success?
Identify the key metrics that will indicate the success of your experiment. These could be conversion rates, click-through rates, or other relevant performance indicators. Ensure these metrics are aligned with your defined goals.

Want to know more about defining success events with Gleef? Visit our [success event page.](/how-to/success-event)

---
## Creating variations

### Can you remove the default text?
For effective A/B testing, retain the default text as the baseline of your experiment. This serves as the reference point for evaluating the performance of other variations.

### How many variations should you test?
To keep your experiment manageable, limit the number of variations to two or three. Testing too many variations can complicate analysis and require a larger sample size to achieve statistical significance.

<Info>
    Keep in mind that you’ll need around 7,000 users per variation to detect a 1% change in performance relative to your baseline. <br/>
    Learn more about [statistical significance here.](/Q&A/statistics-data#statistical-significance)
</Info>

### What kind of wording should you experiment with?
Focus on high-impact elements such as headlines, call-to-action buttons, and product descriptions. Small changes, like tweaking wording of a headline or button, can lead to substantial improvements in user engagement.

### Why can't I run any UI experiments with Gleef?
Gleef specializes in texts and words within your digital products. We firmly believe that testing UI is a crucial aspect of your website or application and should precede any wording A/B tests.
Our goal is to make A/B testing straightforward, easy, and entirely no-code, ensuring no impact on your website structure. Be cautious with no-code tools for UI A/B testing, as they can potentially disrupt your website.

We deeply believe in the power of words and aim to help you experiment with optimal wordings effortlessly. In the near future, we also plan to assist you in selecting the right words every single time.

---

## Running experiments

### How long should you run your experiments?
Run your experiments for a duration that allows you to gather ample data to reach statistical significance. This timeframe can vary based on your traffic volume but running the test for at least one to two weeks is a common practice.


### How do you ensure your experiments are not biased?
- Run the experiment for more than one week to mitigate the impact of any specific external event or cause.
- Ensure no changes are made to the webpage during the experiment that could interfere with the results.
- Avoid running multiple overlapping experiments on the same page to prevent skewed results. Isolate each experiment to a specific section of your website to ensure that changes are the only variable impacting user behavior.

<Tip>Don't worry, Gleef also manages multiple experiments on the same page. Learn how [here](/Q&A/QA#running-multiple-experiments-on-the-same-page)</Tip>

### How many experiments can be run at the same time?
- There is no limitation on the number of experiments that can run simultaneously across the overall website.
- On a specific page, avoid running multiple overlapping experiments to prevent skewed results. Isolate each experiment to a specific section of your website to ensure that changes are the only variable impacting user behavior.

<Tip>Don't worry, Gleef also manages multiple experiments on the same page. Learn how [here](/Q&A/QA#running-multiple-experiments-on-the-same-page)</Tip>


---
## Analyzing results

### What statistical methods should you use?
Use standard statistical methods to analyze your results. Calculate the confidence interval and p-value to determine if the outcomes are statistically significant. Aim for a 95% confidence level to ensure reliability.

<Info>We provide detailed explanations on statistical calculations. Refer to our [statistical Q&A for additional information.](/Q&A/statistics-data)</Info>

### How do you interpret the results?
Look beyond simply identifying the winning variation. Analyze why a certain variation performed better and how it influenced user behavior. These insights can inform future experiments and facilitate overall website optimization.

---

## Maintaining consistency

### Avoid making any changes to the page during the experiment
Always coordinate with your development team to ensure no changes are made to the page while an experiment is active. Any modifications can disrupt the experiment, skew results, and introduce bias.

### Avoid altering the experiment while it's running
Introducing or changing variations or the success event during the experiment can lead to inaccurate results. If modifications are necessary, design and implement a new experiment – it’s quick and easy with Gleef. Mid-experiment changes will introduce bias and render the results unreliable.

---

## Avoiding common pitfalls

### Changing multiple elements at once
To accurately attribute performance differences, change only one element at a time.
### Running tests without enough traffic
Ensure you have sufficient traffic to reach statistical significance.
### Stopping tests too early
Allow the experiment enough time to gather reliable and actionable data.

---

## Conclusion
A/B testing is a powerful tool for enhancing website performance, but it requires careful execution to ensure valid results. By defining clear objectives, creating relevant variations, running unbiased experiments, and accurately analyzing results, you can make data-driven decisions that genuinely improve user engagement and conversion rates.