---
title: 'Best Practices'
icon: 'circle-check'
iconType: 'solid'
---

A/B testing wording can dramatically enhance user engagement and conversion rates. Leading tech-savvy companies have launched their own A/B testing tools over recent years, conducting thousands of experiments annually.

<Tip>
    Did you know that Netflix continuously runs on average 20 A/B tests for every user at any given time?<br/>
    <i>Learn more at the [Netflix Technology Blog](https://netflixtechblog.medium.com)</i>
</Tip>

To ensure your experiments are both effective and insightful, adhere to these best practices.

---
## Defining goals

### What is the objective of your A/B test?
Clearly define the objective of your experiment. Common goals include increasing click-through rates, improving conversion rates, and reducing bounce rates. A well-defined goal helps design relevant variations and measure success accurately.

While A/B testing is commonly used to increase revenue, it can also be employed for various other objectives:
- Increasing upsells
- Reducing opt-out rates
- Reducing after-sales contact
- Improving community engagement
- Reducing time to purchase

<Tip>
    Don't just focus on increasing revenue. A/B testing can also significantly help in reducing your costs. <br/>
    Discover more through our [user stories.](/Documentation/Examples)
</Tip>

### How will you measure success?
Identify the key metrics that will indicate the success of your experiment. These could be conversion rates, click-through rates, or other relevant performance indicators. Ensure these metrics are aligned with your defined goals.

Want to know more about defining success events with Gleef? Visit our [success event page.](/how-to/success-event)

---
## Creating variations

### Can you remove the default text?
For effective A/B testing, retain the default text as the baseline of your experiment. This serves as the reference point for evaluating the performance of other variations.

### How many variations should you test?
To keep your experiment manageable, limit the number of variations to two or three. Testing too many variations can complicate analysis and require a larger sample size to achieve statistical significance.

<Info>
    Keep in mind that you‚Äôll need around 7,000 users per variation to detect a 1% change in performance relative to your baseline. <br/>
    Learn more about [statistical significance here.](/Q&A/statistics-data#statistical-significance)
</Info>

### What kind of wording should you experiment with?
Focus on high-impact elements such as headlines, call-to-action buttons, and product descriptions. Small changes, like tweaking wording of a headline or button, can lead to substantial improvements in user engagement.

### Why can't I run any UI experiments with Gleef?
Gleef specializes in texts and words within your digital products. We firmly believe that testing UI is a crucial aspect of your website or application and should precede any wording A/B tests.
Our goal is to make A/B testing straightforward, easy, and entirely no-code, ensuring no impact on your website structure. Be cautious with no-code tools for UI A/B testing, as they can potentially disrupt your website.

We deeply believe in the power of words and aim to help you experiment with optimal wordings effortlessly. In the near future, we also plan to assist you in selecting the right words every single time.

---

## Running experiments

### How long should you run your experiments?
Run your experiments for a duration that allows you to gather ample data to reach statistical significance. This timeframe can vary based on your traffic volume but running the test for at least one to two weeks is a common practice.


### How do you ensure your experiments are not biased?
- Run the experiment for more than one week to mitigate the impact of any specific external event or cause.
- Ensure no changes are made to the webpage during the experiment that could interfere with the results.
- Avoid running multiple overlapping experiments on the same page to prevent skewed results. Isolate each experiment to a specific section of your website to ensure that changes are the only variable impacting user behavior.

<Tip>Don't worry, Gleef also manages multiple experiments on the same page. Learn how [here](/Q&A/QA#running-multiple-experiments-on-the-same-page)</Tip>

### How many experiments can be run at the same time?
- There is no limitation on the number of experiments that can run simultaneously across the overall website.
- On a specific page, avoid running multiple overlapping experiments to prevent skewed results. Isolate each experiment to a specific section of your website to ensure that changes are the only variable impacting user behavior.

<Tip>Don't worry, Gleef also manages multiple experiments on the same page. Learn how [here](/Q&A/QA#running-multiple-experiments-on-the-same-page)</Tip>


---
## Analyzing results

### What statistical methods should you use?
Use standard statistical methods to analyze your results. Calculate the confidence interval and p-value to determine if the outcomes are statistically significant. Aim for a 95% confidence level to ensure reliability.

<Info>We provide detailed explanations on statistical calculations. Refer to our [statistical Q&A for additional information.](/Q&A/statistics-data)</Info>

### How do you interpret the results?
Look beyond simply identifying the winning variation. Analyze why a certain variation performed better and how it influenced user behavior. These insights can inform future experiments and facilitate overall website optimization.

---

## Maintaining consistency

### Avoid making any changes to the page during the experiment
Always coordinate with your development team to ensure no changes are made to the page while an experiment is active. Any modifications can disrupt the experiment, skew results, and introduce bias.

### Avoid altering the experiment while it's running
Introducing or changing variations or the success event during the experiment can lead to inaccurate results. If modifications are necessary, design and implement a new experiment ‚Äì it‚Äôs quick and easy with Gleef. Mid-experiment changes will introduce bias and render the results unreliable.

---

## Avoiding common pitfalls

### Changing multiple elements at once
To accurately attribute performance differences, change only one element at a time.
### Running tests without enough traffic
Ensure you have sufficient traffic to reach statistical significance.
### Stopping tests too early
Allow the experiment enough time to gather reliable and actionable data.

---

## Conclusion
A/B testing is a powerful tool for enhancing website performance, but it requires careful execution to ensure valid results. By defining clear objectives, creating relevant variations, running unbiased experiments, and accurately analyzing results, you can make data-driven decisions that genuinely improve user engagement and conversion rates.






Statistical significance is based on various parameters. Gleef uses an automatic calculation to determine whether your results are significant.
For detailed insight on how this is done, refer to our [statistics Q&A](/Q&A/statistics-data).

Continue running the experiment until you achieve statistical significance. Ending the experiment prematurely could hinder the reliability of your results. Remember, significance is closely related to the magnitude of the conversion difference between variations.
For example, if you have a 10% conversion difference between two variations, your experiment may reach significance much faster than if the difference were only 1%.

<Note>Significance is fixed at 95% for all experiments, and currently, it is not customizable.</Note>

For the moment, Gleef does not offer the possibility of managing the significance parameters of experiments. This is automatically triggered when the 95% confidence level is reached (p-value of 0.05). You can see the critical Z value directly in the console when you open the dashboard on a specific experiment. As a reminder, the values are as follows:

| Confidence Level | Critical Value, ùëçùëê |
| -------- | -------------------------- |
| 99%      | 2.575                      |
| 95%      | 2.33                       |
| 90%      | 1.645                      |
| 80%      | 1.28                       |

The console displays the critical Z value and the p-value.

<Tip>Please note that the significance of experiments is calculated purely from a mathematical point of view for the time being, and no other rule is involved in changing the dot determining the significance of the experiment.
However, we recommend that you achieve the following values to ensure the validity of the experiment.</Tip>

Value to look at on top of statistical significance, before taking any decision on your wording:
- At least 5,000 unique visitors per variation;
- To run the test for at least 14 days (two business cycles);
- Achieve 250 conversions.

### Conversion & growth
<img width="200" src="/images/monitor_results.png" alt="Results"/>
At any time, you can access and monitor the results of your experiment, observing the conversion rates of each variation, and the improvement compared to the `baseline`, which represents the original wording being tested.

<Tip>
    Hover over the conversion or growth metrics for detailed figures.
    <img width="200" src="/images/monitor_hover.png" alt="Hover conversion"/>
</Tip>
